{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization: Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick demo for concepts related to Matrix Factorization for Recommender Systems, inspired by the [Colab Notebook](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb) of the [\"Recommendation Systems\" course](https://developers.google.com/machine-learning/recommendation) created by some kind folks at Google. It uses Google's TensorFlow library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "The goal in Matrix Factorization is to find two matrices, a user matrix $U$ and an item matrix $V$, each having the same number of columns. Multiplied together, those two matrices should approximate some \"target matrix\" $A$, i.e. those two matrices are *factors* that should (roughly) reproduce $A$ when multiplied together. [This](https://youtu.be/ZspR5PZemcs) video explains the general concept very nicely.\n",
    "\n",
    "The items and users are described by so-called *embeddings*. Note that the \"variables\" used for embeddings are the same for both the users and the items! In our case, they are very simple: just three numbers. \n",
    "\n",
    "What could those numbers mean? This is very context-dependent. In the case of a movie platform they could be \"movie/user profiles\", for example three common movie categories. A higher value for a particular variable for a user in $U$ could mean that the user likes this particular type of movie a lot. Similarly, a high value for a particular variable for a movie in $V$ could mean that this movie fits the corresponding genre especially well. \n",
    "\n",
    "In our example, both $U$ and $V$ have $d = 3$ columns, while $U$ has $n$ rows and $V$ has $m$ rows. Consequently, $A$ (which should be approximated by $U \\cdot V^T$) has shape $n \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required: TensorFlow 2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define very simple example matrices for the rating matrix $A$ and its \"factor matrices\", $U$ and $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-31 17:41:57.340545: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-31 17:41:57.341922: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "# we need to turn on eager execution to be able to see resulting tensors \"live\", without needing to call eval() (by calling numpy() on the computed tensors)\n",
    "# more on eager execution and graphs: https://www.tensorflow.org/guide/intro_to_graphs\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "U = tf.constant([[3.0, 4.0, 5.0], [2.0, 5.0, 3.0]])\n",
    "V = tf.constant([[3.0, 3.0, 3.0], [5.0, 1.0, 3.0], [1.0, 5.0, 5.0]])\n",
    "A = tf.sparse.SparseTensor(\n",
    "  indices = [[0, 2], [1, 0], [1, 2]],\n",
    "  values = [44.0, 32.0, 38.0],\n",
    "  dense_shape = [2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While executing TensorFlow code eagerly, Tensor's have a `numpy()` function that we can use to inspect their contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 4., 5.],\n",
       "       [2., 5., 3.]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3., 3.],\n",
       "       [5., 1., 3.],\n",
       "       [1., 5., 5.]], dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For printing sparse tensors, we can make use of the following utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_sparse_tensor(st):\n",
    "  s = \"<SparseTensor\\n shape = %s \\n values = {\" % (st.dense_shape.numpy().tolist(),)\n",
    "  for (index, value) in zip(st.indices.numpy(), st.values.numpy()):\n",
    "    s += f\"\\n  %s: %s,\" % (index.tolist(), value.tolist())\n",
    "  print(s + \"\\n }>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sparse matrix we would like to approximate with $U \\cdot V^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparseTensor\n",
      " shape = [2, 3] \n",
      " values = {\n",
      "  [0, 2]: 44.0,\n",
      "  [1, 0]: 32.0,\n",
      "  [1, 2]: 38.0,\n",
      " }>\n"
     ]
    }
   ],
   "source": [
    "pprint_sparse_tensor(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating quality of Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, an \"ideal\" Matrix Factorization is learned by Machine Learning algorithms. As every Machine Learning algorithm needs some loss function to optimize, we will also use one in this example. We will go with the Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computing \"prediction\" (using $U$ and $V$) for existing elements in $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goal is achieved by multiplying $U$ with $V^T$ and keeping only entries that were non-zero in $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpler (less efficient) approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matrix multiplication and then subset with `tf.gather()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36., 34., 48.],\n",
       "       [30., 24., 42.]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_product = U @ tf.transpose(V)\n",
    "matrix_product.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather_nd() allows us to pick selected elements from a multi-dimensional tensor via a multi-dimensional tensor of indices\n",
    "result_a = tf.gather_nd(matrix_product, A.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48., 30., 42.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More efficient approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only take those pairs of elements from $U$ and $V$ into account that actually appear in the original matrix $A$, so we skip all cases where we would multiply with 0 (or NaN). In practice this makes a lot of sense, as most users have rated only a tiny fraction of all the available items, i.e. the matrix $A$ is very sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indices_U = A.indices[:, 0]\n",
    "match_indices_V = A.indices[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_indices_U.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_indices_V.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this make sense? Compare with the non-zero indices of $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 0],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: finish\n",
    "#  def sparse_2d_tensor_to_dense_2d_array(sparse):\n",
    "#   matrix = np.ndarray(shape = sparse.dense_shape.numpy())\n",
    "#   for i, pos in enumerate(sparse.indices.numpy().tolist()):\n",
    "#     matrix[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the elements we would consider are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather() allows us to pick selected elements from a tensor via a tensor of indices\n",
    "match_partners_U = tf.gather(U, match_indices_U)\n",
    "match_partners_V = tf.gather(V, match_indices_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do element-wise multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementwise = match_partners_U * match_partners_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3., 20., 25.],\n",
       "       [ 6., 15.,  9.],\n",
       "       [ 2., 25., 15.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elementwise.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, sum by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_b = tf.reduce_sum(elementwise, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48., 30., 42.], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is identical to the other approach, nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing MSE from prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48., 30., 42.], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = result_a # result_b is identical\n",
    "prediction.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual (non-zero) entries in the matrix were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44., 32., 38.], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.values.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the MSE, we now just sum up the squared differences for each vector element and divide by the length of the two vectors (or tensors, to be exact). TensorFlow has a utility function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.losses.mean_squared_error(A.values, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes sense, we were off by 2 each time, 3 * 2^2 = 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting everything into a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sparse_mean_square_error(sparse_ratings, user_embeddings, movie_embeddings):\n",
    "  \"\"\"\n",
    "  Args:\n",
    "    sparse_ratings: A SparseTensor rating matrix, of dense_shape [N, M]\n",
    "    user_embeddings: A dense Tensor U of shape [N, k] where k is the embedding\n",
    "      dimension, such that U_i is the embedding of user i.\n",
    "    movie_embeddings: A dense Tensor V of shape [M, k] where k is the embedding\n",
    "      dimension, such that V_j is the embedding of movie j.\n",
    "  Returns:\n",
    "    A scalar Tensor representing the MSE between the true ratings and the\n",
    "      model's predictions.\n",
    "  \"\"\"\n",
    "  predictions = tf.reduce_sum(\n",
    "      tf.gather(user_embeddings, sparse_ratings.indices[:, 0]) *\n",
    "      tf.gather(movie_embeddings, sparse_ratings.indices[:, 1]),\n",
    "      axis=1)\n",
    "  loss = tf.losses.mean_squared_error(sparse_ratings.values, predictions)\n",
    "  return loss\n",
    "\n",
    "sparse_mean_square_error(A, U, V).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Matrix Factorization in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find \"best\" Matrix Factorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a ML model (most probably a neural network) would \"learn\" a good matrix factorization over several iterations, each time trying to minimize the loss function. In neural networks, \"Stochastic Gradient Descent\" is commonly used for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use for recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of Matrix factorization is to use it for predictions on new, unseen data. The matrix factorization $U \\cdot V^T$ can be thought of as a \"score matrix\" for item/user combinations. By learning a good approximation for $A$, we implicitly also learn \"expected scores\" for combinations of users and items that didn't occur up until now. This means, that for recommending a new item to a user, we can simply have a look at that user's row in $U \\cdot V^T$ and pick the element with the highest score! The index of that element is the item we should try to recommend first!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using regularization to improve recommendations from Matrix Factorization model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted in the the [Colab Notebook](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb) of Google's [\"Recommendation Systems\" course](https://developers.google.com/machine-learning/recommendation), using the MSE on the observed part of the rating matrix $A$ may result in poor model performance, as the model does not learn how to place the embeddings of irrelevant items. This phenomenon is known as *folding*.\n",
    "\n",
    "We will add regularization terms that will address this issue. We will use two types of regularization:\n",
    "- Regularization of the model parameters. This is a common $\\ell_2$ regularization term on the embedding matrices, given by $r(U, V) =  \\frac{1}{N} \\sum_i \\|U_i\\|^2 + \\frac{1}{M}\\sum_j \\|V_j\\|^2$.  \n",
    "*Note: We use $\\|a\\|$ to denote the *norm* of a vector $a$. So, $r(U,V)$ is actually a penalty on the magnitude of the rows of $U$ and $V$, i. e. the user/item embeddings in the embedding matrices.*\n",
    "- A global prior that pushes the prediction of any pair towards zero, called the *gravity* term. This is given by $g(U, V) = \\frac{1}{MN} \\sum_{i = 1}^N \\sum_{j = 1}^M \\langle U_i, V_j \\rangle^2$.\n",
    "\n",
    "The total loss is then given by\n",
    "$$\n",
    "\\frac{1}{|\\Omega|}\\sum_{(i, j) \\in \\Omega} (A_{ij} - \\langle U_i, V_j\\rangle)^2 + \\lambda _r r(U, V) + \\lambda_g g(U, V)\n",
    "$$\n",
    "where $\\lambda_r$ and $\\lambda_g$ are two regularization coefficients (hyper-parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can express $r(U,V)$ and $g(U, V)$ in code like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularization(U, V):\n",
    "  return (tf.reduce_sum(U * U) / (U.shape[0])  + \n",
    "          tf.reduce_sum(V * V) / (V.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gravity(U, V):\n",
    "  \"\"\"Creates a gravity loss given two embedding matrices.\"\"\"\n",
    "  return 1. / (U.shape[0] * V.shape[0]) * tf.reduce_sum(\n",
    "      tf.matmul(U, U, transpose_a=True) * tf.matmul(V, V, transpose_a=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would then define the regularization and gravity coefficients ${\\lambda}_r$ and ${\\lambda}_g$ as hyperparameters before training and compute the loss in each epoch/iteration of the training process with a function that sums the three losses/error terms together to get the \"total loss\". The computation could look something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "regularization_coeff = 0.1\n",
    "gravity_coeff = 1.0\n",
    "\n",
    "def get_regularized_loss(A, U, V):\n",
    "  return sparse_mean_square_error(A, U, V) + regularization_coeff * regularization(U, V) + gravity_coeff * gravity(U, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1352.8334"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_regularized_loss(A, U, V).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the loss is *much* larger, as the values of $U$ and $V$ are quite large. I am also not so sure if the example values I picked for the entries in $A$ even make much sense. It seems to me as if neural network weights are often initialized with values following a standard normal distribution (which is centered around 0)? At least then the *gravity term* that pushes the prediction of any pair towards zero would also make more sense. But I must admit that I don't understand the topic well enough at this point. For a more meaningful discussion of results of non-regularized vs. regularized Matrix Factorization models see also the notebook I reference above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aff2677908d05d84f1d8db7cda71d16fe8413bdcab85e14a16c11d1442f2d5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('recsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
