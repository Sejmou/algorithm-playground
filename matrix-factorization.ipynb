{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Factorization: Toy Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick demo for concepts related to Matrix Factorization for Recommender Systems, inspired by the [Colab Notebook](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/recommendation-systems/recommendation-systems.ipynb) of the [\"Recommendation Systems\" course](https://developers.google.com/machine-learning/recommendation) created by some kind folks at Google. It uses Google's TensorFlow library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scenario\n",
    "The goal in Matrix Factorization is to find two matrices, a user matrix $U$ and an item matrix $V$, each having the same number of columns. Multiplied together, those two matrices should approximate some \"target matrix\" $A$, i.e. those two matrices are *factors* that should (roughly) reproduce $A$ when multiplied together. [This](https://youtu.be/ZspR5PZemcs) video explains the general concept very nicely.\n",
    "\n",
    "The items and users are described by so-called *embeddings*. Note that the \"variables\" used for embeddings are the same for both the users and the items! In our case, they are very simple: just three numbers. \n",
    "\n",
    "What could those numbers mean? This is very context-dependent. In the case of a movie platform they could be \"movie/user profiles\", for example three common movie categories. A higher value for a particular variable for a user in $U$ could mean that the user likes this particular type of movie a lot. Similarly, a high value for a particular variable for a movie in $V$ could mean that this movie fits the corresponding genre especially well. \n",
    "\n",
    "In our example, both $U$ and $V$ have $d = 3$ columns, while $U$ has $n$ rows and $V$ has $m$ rows. Consequently, $A$ (which should be approximated by $U \\cdot V^T$) has shape $n \\times m$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required: TensorFlow 2\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define very simple example matrices for the rating matrix $A$ and its \"factor matrices\", $U$ and $V$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to turn on eager execution to be able to see resulting tensors \"live\", without needing to call eval() (by calling numpy() on the computed tensors)\n",
    "# more on eager execution and graphs: https://www.tensorflow.org/guide/intro_to_graphs\n",
    "tf.config.run_functions_eagerly(False)\n",
    "\n",
    "U = tf.constant([[3, 4, 5], [2, 5, 3]])\n",
    "V = tf.constant([[3, 3, 3], [5, 1, 3], [1, 5, 5]])\n",
    "A = tf.sparse.SparseTensor(\n",
    "  indices = [[0, 2], [1, 0], [1, 2]],\n",
    "  values = [44, 32, 38],\n",
    "  dense_shape = [2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While executing TensorFlow code eagerly, Tensor's have a `numpy()` function that we can use to inspect their contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 5],\n",
       "       [2, 5, 3]], dtype=int32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 3, 3],\n",
       "       [5, 1, 3],\n",
       "       [1, 5, 5]], dtype=int32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For printing sparse tensors, we can make use of the following utility function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_sparse_tensor(st):\n",
    "  s = \"<SparseTensor\\n shape = %s \\n values = {\" % (st.dense_shape.numpy().tolist(),)\n",
    "  for (index, value) in zip(st.indices.numpy(), st.values.numpy()):\n",
    "    s += f\"\\n  %s: %s,\" % (index.tolist(), value.tolist())\n",
    "  print(s + \"\\n }>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the sparse matrix we would like to approximate with $U \\cdot V^T$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparseTensor\n",
      " shape = [2, 3] \n",
      " values = {\n",
      "  [0, 2]: 44,\n",
      "  [1, 0]: 32,\n",
      "  [1, 2]: 38,\n",
      " }>\n"
     ]
    }
   ],
   "source": [
    "pprint_sparse_tensor(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating quality of Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, an \"ideal\" Matrix Factorization is learned by Machine Learning algorithms. As every Machine Learning algorithm needs some loss function to optimize, we will also use one in this example. We will go with the Mean Squared Error (MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Computing \"prediction\" (using $U$ and $V$) for existing elements in $A$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This goal is achieved by multiplying $U$ with $V^T$ and keeping only entries that were non-zero in $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simpler (less efficient) approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matrix multiplication and then subset with `tf.gather()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36, 34, 48],\n",
       "       [30, 24, 42]], dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_product = U @ tf.transpose(V)\n",
    "matrix_product.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather_nd() allows us to pick selected elements from a multi-dimensional tensor via a multi-dimensional tensor of indices\n",
    "result_a = tf.gather_nd(matrix_product, A.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 30, 42], dtype=int32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_a.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More efficient approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only take those pairs of elements from $U$ and $V$ into account that actually appear in the original matrix $A$, so we skip all cases where we would multiply with 0 (or NaN). In practice this makes a lot of sense, as most users have rated only a tiny fraction of all the available items, i.e. the matrix $A$ is very sparse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_indices_U = A.indices[:, 0]\n",
    "match_indices_V = A.indices[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_indices_U.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 2])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_indices_V.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this make sense? Compare with the non-zero indices of $A$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 2],\n",
       "       [1, 0],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.indices.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  TODO: finish\n",
    "#  def sparse_2d_tensor_to_dense_2d_array(sparse):\n",
    "#   matrix = np.ndarray(shape = sparse.dense_shape.numpy())\n",
    "#   for i, pos in enumerate(sparse.indices.numpy().tolist()):\n",
    "#     matrix[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the elements we would consider are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather() allows us to pick selected elements from a tensor via a tensor of indices\n",
    "match_partners_U = tf.gather(U, match_indices_U)\n",
    "match_partners_V = tf.gather(V, match_indices_V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we do element-wise multiplication:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementwise = match_partners_U * match_partners_V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3, 20, 25],\n",
       "       [ 6, 15,  9],\n",
       "       [ 2, 25, 15]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elementwise.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, finally, sum by row:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_b = tf.reduce_sum(elementwise, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 30, 42], dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is identical to the other approach, nice!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Computing MSE from prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our prediction was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48, 30, 42], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = result_a # result_b is identical\n",
    "prediction.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual (non-zero) entries in the matrix were:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([44, 32, 38], dtype=int32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.values.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the MSE, we now just sum up the squared differences for each vector element and divide by the length of the two vectors (or tensors, to be exact). TensorFlow has a utility function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = tf.compat.v1.losses.mean_squared_error(A.values, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes sense, we were off by 2 each time, 3 * 2^2 = 12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage of Matrix Factorization in practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to find \"best\" Matrix Factorization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, a ML model (most probably a neural network) would \"learn\" a good matrix factorization over several iterations, each time trying to minimize the loss function. In neural networks, \"Stochastic Gradient Descent\" is commonly used for that purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use for recommendations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole purpose of Matrix factorization is to use it for predictions on new, unseen data. The matrix factorization $U \\cdot V^T$ can be thought of as a \"score matrix\" for item/user combinations. By learning a good approximation for $A$, we implicitly also learn \"expected scores\" for combinations of users and items that didn't occur up until now. This means, that for recommending a new item to a user, we can simply have a look at that user's row in $U \\cdot V^T$ and pick the element with the highest score! The index of that element is the item we should try to recommend first!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aff2677908d05d84f1d8db7cda71d16fe8413bdcab85e14a16c11d1442f2d5c6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('recsys')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
